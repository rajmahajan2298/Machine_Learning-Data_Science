Supervised Learnig --> Have Dependended Features
-----> Output Feature
-----> Categorical ---> Classification
-----> Contnious ---> Regression

Binary Classification ---> Two Category
Mutlilevel ---> Two or more Category

Unsupervised Learning ---> No Output Features
------> We are Creating Clussters in this 

Reinforcement Learning 
-----> The Application will learns the things by itself

Equation of Straight line -

y = mx+c
ax+by+c = 0

wTx + b = 0
wTx = 0

distacne from a plane at top = +ve
distance from a plane at bottom = -ve

Instance based learning ----> Memorising
Model based learning -----> Generlizing

Simple Linearing Regression
----> The Main AIM is to create a best fit line so that the summiation of actaul and predicted point
must be minimal.

We can create best fit line by using cost functions

Types of Cost Function
(i)MSE ---> Mean Squared Error
(ii)RMSE ---> Root Mean Squared Error
(iii)MAE ---> Absloute Mean Error

Global Minima ---> We need to come to a point know as Global Minima
by changing the value of 01 and 02 the curve it is found is known as
Gradient Descent

Convergence Algoithm ---> Help how to change the value of deta to reach global
minima.

To find the derivate by using convergence algorithm we create a tangent line.
If the right side of the tanget line facing towards the downwards we can say 
it is a (-ve slope) and we need to increase the alpha value.
If it is facing upwards we can say it is a (+ve slope) and we need to decrease
the value
The Alpha in Convergence Equation is Learning rate.
It shows how our algorithm or at what speed it is learning.

Multiple Linear Regression 
----> We have Multiple Input features
----> Multiple slope
----> One Intercept

Performance Matrix
(i)R-Squared
(ii) Adjusted R-Squared

R-Squared ----> Measures the praportion of the variance in the dependent
variable(y) that is predictable from the independent variable (x).

R-Squared = 1 - (SSE / SST)

SSE = Sum of Square of Errors
SST = Sum of Square of Total

if R-Squared values indicated better model fit is more.

Adjusted R-Squared ---> Modifies R-Squared to account of the number
of Predictors in the model.

Adjusted R-Squared = 1 -(SSE /(n-k-1)) / (SST / (n - 1))
n =  sample size
k = number of predictors

When to use :-

R-Squared is for simple models or intial evalution.
Adjusted R-Squared for more complex models or final evalutions.

(i)MSE
(ii) MAE
(iii) RMSE

Overfitting and Underfitting

OLS -----> Ordinary Least Square

B0 = y'-b1xi
B1 = (yi-y')/(xi-x')

Multiple Linear Regression Implemeatation
Polnomial Linear Regression

Implementation of Polynomial and Multiple Linear Regression.
Pipeline in Polynomial

Handling Missing Values

Non-Linear Relationships


